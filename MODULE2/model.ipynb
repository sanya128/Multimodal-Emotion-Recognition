{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/gdrive/\")\n","import os\n","os.chdir(\"/content/gdrive/My Drive/MultimodalEmotion/MODULE2\")\n","!ls"],"metadata":{"id":"4Zcsbi2hqap9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740648232698,"user_tz":-330,"elapsed":2135,"user":{"displayName":"VIRENDRA KUMAR MEGHWAL","userId":"10987795721038852225"}},"outputId":"0c61be9b-8a78-477b-b035-03550f9e6579"},"id":"4Zcsbi2hqap9","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n","angry.jpeg\t\t fer_model.h5\t\t\t      model.ipynb\n","data\t\t\t haarcascade_frontalface_default.xml  neutral.jpeg\n","emotion_detection.ipynb  happy.jpeg\t\t\t      sad.jpeg\n","fear.jpg\t\t image_emotion.npy\t\t      test_image_data.npy\n"]}]},{"cell_type":"code","execution_count":12,"id":"d0504b67","metadata":{"id":"d0504b67","executionInfo":{"status":"ok","timestamp":1740648232778,"user_tz":-330,"elapsed":68,"user":{"displayName":"VIRENDRA KUMAR MEGHWAL","userId":"10987795721038852225"}}},"outputs":[],"source":["import numpy as np\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n","from keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":13,"id":"cb3f3f82","metadata":{"id":"cb3f3f82","executionInfo":{"status":"ok","timestamp":1740648232781,"user_tz":-330,"elapsed":18,"user":{"displayName":"VIRENDRA KUMAR MEGHWAL","userId":"10987795721038852225"}}},"outputs":[],"source":["# Set the dimensions of the input images\n","img_width, img_height = 48, 48\n","\n","# Set the directories for the training and validation datasets\n","train_data_dir = '/data/train'\n","validation_data_dir = '/data/test'\n","\n","# Set the number of training and validation samples\n","nb_train_samples = 28709\n","nb_validation_samples = 3589\n","\n","# Set the batch size and number of epochs\n","batch_size = 64\n","epochs = 50\n","\n","# Set the number of classes (emotions)\n","num_classes = 7"]},{"cell_type":"code","execution_count":14,"id":"625a8eaf","metadata":{"id":"625a8eaf","executionInfo":{"status":"ok","timestamp":1740648232783,"user_tz":-330,"elapsed":9,"user":{"displayName":"VIRENDRA KUMAR MEGHWAL","userId":"10987795721038852225"}}},"outputs":[],"source":["np.save(\"test_image_data.npy\", validation_data_dir)"]},{"cell_type":"code","execution_count":15,"id":"28784df0","metadata":{"id":"28784df0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740648232874,"user_tz":-330,"elapsed":98,"user":{"displayName":"VIRENDRA KUMAR MEGHWAL","userId":"10987795721038852225"}},"outputId":"e670d086-e9d6-4f9f-f106-6683e05b55e8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}],"source":["# Create the model\n","model = Sequential()\n","\n","model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 1)))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Conv2D(128, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(1024, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"code","execution_count":16,"id":"5e72090c","metadata":{"id":"5e72090c","outputId":"d31a9930-5695-4f04-bee6-e521700e00ff","colab":{"base_uri":"https://localhost:8080/","height":468},"executionInfo":{"status":"error","timestamp":1740648232927,"user_tz":-330,"elapsed":50,"user":{"displayName":"VIRENDRA KUMAR MEGHWAL","userId":"10987795721038852225"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/data/train'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-da7a4e902af6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalidation_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_datagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mvalidation_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_datagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/train'"]}],"source":["# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001, decay=1e-6), metrics=['accuracy'])\n","\n","# Create the data generators for the training and validation datasets\n","train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n","validation_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(train_data_dir, color_mode='grayscale', target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical')\n","validation_generator = validation_datagen.flow_from_directory(validation_data_dir, color_mode='grayscale', target_size=(img_width, img_height), batch_size=batch_size, class_mode='categorical')"]},{"cell_type":"code","execution_count":null,"id":"b9a7c034","metadata":{"id":"b9a7c034","executionInfo":{"status":"aborted","timestamp":1740648233018,"user_tz":-330,"elapsed":49,"user":{"displayName":"VIRENDRA KUMAR MEGHWAL","userId":"10987795721038852225"}}},"outputs":[],"source":["# Train the model\n","model.fit_generator(train_generator, steps_per_epoch=nb_train_samples//batch_size, epochs=epochs, validation_data=validation_generator, validation_steps=nb_validation_samples//batch_size)"]},{"cell_type":"code","execution_count":null,"id":"638925d2","metadata":{"id":"638925d2","executionInfo":{"status":"aborted","timestamp":1740648233022,"user_tz":-330,"elapsed":8,"user":{"displayName":"VIRENDRA KUMAR MEGHWAL","userId":"10987795721038852225"}}},"outputs":[],"source":["print(model.summary())"]},{"cell_type":"code","execution_count":null,"id":"b2db78c7","metadata":{"id":"b2db78c7","executionInfo":{"status":"aborted","timestamp":1740648233024,"user_tz":-330,"elapsed":9,"user":{"displayName":"VIRENDRA KUMAR MEGHWAL","userId":"10987795721038852225"}}},"outputs":[],"source":["# Save the model\n","model.save('fer_model.h5')"]},{"cell_type":"code","execution_count":null,"id":"e08d98e1","metadata":{"id":"e08d98e1","executionInfo":{"status":"aborted","timestamp":1740648233027,"user_tz":-330,"elapsed":10,"user":{"displayName":"VIRENDRA KUMAR MEGHWAL","userId":"10987795721038852225"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}